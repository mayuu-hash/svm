{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "166d556e-6485-4084-a966-f2634f13f69b",
   "metadata": {},
   "source": [
    "# 1. What is Information Gain, and how is it used in Decision Trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfaf553-7487-448d-8744-5303891649cf",
   "metadata": {},
   "source": [
    "-> Information Gain(IG) is a metric used in decision tree algorithms to select the best feature to split the data at a node. It measures how much \"information\" a feature provides by calculating the reduction in entropy (or uncertainty) after the dataset is split on that feature.\n",
    "\n",
    "It is calculated as:\n",
    "Information Gain = Entropy(Parent) - [Weighted Average] * Entropy(Children)\n",
    "\n",
    "Where:\n",
    "\n",
    "· Entropy(Parent) is the impurity of the node before the split.\n",
    "· [Weighted Average] * Entropy(Children) is the sum of the entropy of each child node, weighted by the proportion of samples that reach that child.\n",
    "\n",
    "In decision trees, the algorithm evaluates all possible features and splits at a node and chooses the feature with the highest Information Gain. This process is repeated recursively to build the tree, as it effectively finds the feature that most cleanly separates the classes of the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e513ffc4-0206-4621-b85b-206d2c80612e",
   "metadata": {},
   "source": [
    "# 2.  What is the difference between Gini Impurity and Entropy?\n",
    "-> Gini Impurity and Entropy are both functions used to measure the impurity or disorder of a node in a decision tree.The goal of a split is to reduce this impurity.\n",
    "\n",
    "Feature Gini Impurity Entropy\n",
    "Concept Measures the probability of a random sample being misclassified if it was randomly labeled according to the class distribution in the node. Measures the average amount of \"information\" or \"surprise\" inherent in the node's possible outcomes. A pure node has zero entropy.\n",
    "Calculation  Gini = 1 - \\sum_{i=1}^{C} (p_i)^2   Entropy = - \\sum_{i=1}^{C} p_i * \\log_2(p_i) \n",
    "Range 0 to 0.5 (for binary classification). 0 indicates a perfectly pure node. 0 to 1 (for binary classification). 0 indicates a perfectly pure node.\n",
    "Performance Generally faster to compute as it doesn't involve logarithmic calculations. Slightly slower due to the log computation, but the difference is often negligible.\n",
    "Resulting Tree Tends to create slightly more unbalanced trees by isolating the most frequent class in a branch. Tends to create more balanced trees by producing splits that are more even.\n",
    "Use Cases The default in many libraries (like scikit-learn) and is a good general-purpose choice. Also a very common and effective choice. In practice, the choice between Gini and Entropy often leads to very similar trees and performance.\n",
    "\n",
    "Strengths & Weaknesses: There is no universally superior option. Gini is computationally more efficient, while Entropy might be more sensitive to changes in node probabilities. For most practical applications, the difference in the final model's performance is minimal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21af0aed-769a-49b4-8d9c-eccd77f9978a",
   "metadata": {},
   "source": [
    "# 3. What is Pre-Pruning in Decision Trees?\n",
    "-> Pre-pruning,also known as early stopping, is a technique used to prevent overfitting in decision trees by restricting the growth of the tree during the building phase. Instead of allowing the tree to grow until all leaves are pure (or all data is classified), pre-pruning sets constraints that halt splitting once certain conditions are met.\n",
    "\n",
    "Common pre-pruning parameters include:\n",
    "\n",
    "· max_depth: The maximum allowed depth of the tree.\n",
    "· min_samples_split: The minimum number of samples required to split an internal node.\n",
    "· min_samples_leaf: The minimum number of samples required to be at a leaf node.\n",
    "· max_leaf_nodes: The maximum number of leaf nodes the tree can have.\n",
    "· min_impurity_decrease: A split will only occur if it reduces the impurity by at least this value.\n",
    "\n",
    "The main advantage of pre-pruning is that it is computationally cheaper than post-pruning (which builds the full tree first and then prunes it back), as the tree is never fully expanded. However, a weakness is that it can lead to underfitting if the stopping conditions are too strict, potentially missing important patterns in the data because a \"good\" split might be followed by a \"great\" one that is never discovered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b3ad8c-7b98-4eb7-91d2-5fc26e000298",
   "metadata": {},
   "source": [
    "# 4. Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7532cc75-f645-4d59-8814-b130f4ceb9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances:\n",
      "  sepal length (cm): 0.0000\n",
      "  sepal width (cm): 0.0191\n",
      "  petal length (cm): 0.8933\n",
      "  petal width (cm): 0.0876\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # Feature matrix\n",
    "y = iris.target  # Target vector\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a Decision Tree Classifier with Gini Impurity\n",
    "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Print the feature importances\n",
    "print(\"Feature Importances:\")\n",
    "for feature_name, importance in zip(iris.feature_names, clf.feature_importances_):\n",
    "    print(f\"  {feature_name}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f3c3be-3413-44db-b2cc-b1028051f0f7",
   "metadata": {},
   "source": [
    "# 5. What is Support Vector Machine (SVM)?\n",
    "-> A Support Vector Machine(SVM) is a powerful and versatile supervised machine learning algorithm used for both classification and regression tasks, though it is most commonly associated with classification.\n",
    "\n",
    "The core idea of an SVM classifier is to find the optimal hyperplane in an N-dimensional space (where N is the number of features) that best separates the data points of different classes. This \"optimal hyperplane\" is chosen as the one with the maximum margin, which is the maximum distance between the hyperplane and the nearest data points from any class. These closest data points are called support vectors.\n",
    "\n",
    "Key characteristics:\n",
    "\n",
    "· Maximum Margin: Focuses on the points that are hardest to classify (the support vectors), which often leads to good generalization on unseen data.\n",
    "· Kernel Trick: Can efficiently perform non-linear classification by mapping inputs into high-dimensional feature spaces using kernel functions.\n",
    "· Effectiveness in High Dimensions: Works well in high-dimensional spaces, even when the number of dimensions exceeds the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64be341d-ecec-47f4-801d-28b93100ffd6",
   "metadata": {},
   "source": [
    "# 6. What is the Kernel Trick in SVM?\n",
    "-> The Kernel Trick is a mathematical technique that allows Support Vector Machines(SVMs) to create a non-linear decision boundary without explicitly transforming the input data into a higher-dimensional space.\n",
    "\n",
    "In many cases, data is not linearly separable in its original feature space. The theoretical solution is to map the data to a much higher-dimensional space where a linear separation becomes possible. However, this mapping can be computationally very expensive.\n",
    "\n",
    "The Kernel Trick solves this by using special functions called kernel functions. These functions compute the dot product of the transformed vectors in the high-dimensional space, without ever having to compute the coordinates of the data in that space. They operate directly on the original input vectors.\n",
    "\n",
    "Common kernel functions include:\n",
    "\n",
    "· Linear:  K(x, x') = x \\cdot x'  (for linear separation)\n",
    "· Polynomial:  K(x, x') = (x \\cdot x' + r)^d \n",
    "· Radial Basis Function (RBF):  K(x, x') = \\exp(-\\gamma ||x - x'||^2)  (the most popular for non-linear problems)\n",
    "\n",
    "By using the Kernel Trick, SVMs can efficiently learn complex, non-linear models, making them highly effective for a wide range of problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca097ce-1c51-486f-8963-52de7b0e2061",
   "metadata": {},
   "source": [
    "# 7. Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ae48048-ab43-4180-9ffc-ecfcee406ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM Accuracy: 0.9815\n",
      "RBF SVM Accuracy: 0.9815\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Wine dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize features (Important for SVM)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create SVM classifiers with Linear and RBF kernels\n",
    "svm_linear = SVC(kernel='linear', random_state=42)\n",
    "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "# Train the models\n",
    "svm_linear.fit(X_train_scaled, y_train)\n",
    "svm_rbf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_linear = svm_linear.predict(X_test_scaled)\n",
    "y_pred_rbf = svm_rbf.predict(X_test_scaled)\n",
    "\n",
    "# Calculate and print accuracies\n",
    "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
    "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
    "\n",
    "print(f\"Linear SVM Accuracy: {accuracy_linear:.4f}\")\n",
    "print(f\"RBF SVM Accuracy: {accuracy_rbf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9d79db-4147-46e6-9764-2d977a30c81c",
   "metadata": {},
   "source": [
    "# 8. What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
    "-> The Naïve Bayes classifier is a family of simple probabilistic classifiers based on applying Bayes'Theorem with a strong (naïve) independence assumption between the features.\n",
    "\n",
    "Bayes' Theorem is stated as:\n",
    " P(Y|X) = \\frac{P(X|Y) P(Y)}{P(X)} \n",
    "where:\n",
    "\n",
    "·  P(Y|X)  is the posterior probability of class  Y  given the features  X .\n",
    "·  P(X|Y)  is the likelihood, the probability of the features given class  Y .\n",
    "·  P(Y)  is the prior probability of class  Y .\n",
    "·  P(X)  is the evidence, the probability of the features.\n",
    "\n",
    "The classifier is called \"Naïve\" because it makes a fundamental assumption: it assumes that every feature is conditionally independent of every other feature, given the class label. This means that the presence or absence of one feature does not affect the presence or absence of any other feature.\n",
    "\n",
    "For example, if we are classifying a fruit based on its color, shape, and diameter, a Naïve Bayes classifier would assume that the color being \"red\" does not influence the shape being \"round,\" if the fruit is known to be an apple.\n",
    "\n",
    "Despite this assumption being rarely true in real-world data, Naïve Bayes classifiers often perform surprisingly well. They are very fast, work well with high-dimensional data, and can be effective even with small training datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449c764c-10aa-43c7-8c7d-c2ee79f0ad57",
   "metadata": {},
   "source": [
    "# 9. Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes\n",
    "-> The different variants of Naïve Bayes classifiers are primarily distinguished by the type of data they are designed to handle and the distribution they assume for the features.\n",
    "\n",
    "Variant Data Type Distribution Assumption Use Case\n",
    "Gaussian Naïve Bayes Continuous, real-valued data. Assumes that the continuous values associated with each class are distributed according to a Gaussian (Normal) distribution. Classification with features like measurements (e.g., height, weight, pixel intensity).\n",
    "Multinomial Naïve Bayes Discrete data, especially counts. Assumes features are generated from a multinomial distribution. This is the standard version for text classification. Text classification (e.g., spam detection, sentiment analysis) where features are word counts or term frequencies (TF-IDF).\n",
    "Bernoulli Naïve Bayes Binary / Boolean features. Assumes that all features are independent binary (Bernoulli) variables. It penalizes the non-occurrence of a feature that is indicative of a class. Text classification with binary term occurrence (e.g., 1 if a word is present in a document, 0 otherwise). Also useful for other datasets with binary features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f693d2-e4cb-4d9c-86c6-d5ed7f260591",
   "metadata": {},
   "source": [
    "# 10. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
